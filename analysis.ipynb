{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Bias amongst long-time readers towards favored authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The following notebook is the final submission of DATA 512 project, and it hopes to evaluate and measure the biases amongst reviewers against authors, specifically in the case of Books, sold and reviewed, on Amazon.com ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goals & Motivation:\n",
    "\n",
    "The primary objective of the research to evaluate confirmation biases amongst real world users. At this stage, the project aims to develop metrics to aptly identify these biases and their effects. The research's other motive is to create accessible research, that can be understood by experts in fields not related to machine learning.\n",
    "\n",
    "#### Why this is important?\n",
    "\n",
    "The internet has become a very nasty place over the years, and co-ordinated efforts to defame/take down a given person are fairly common, for the better or for the for the worse. Online reviews are at the heart of this matter, giving users an inordinate amount of power, which on a daily basis leads to a chaotic mixture of good and bad. On one hand, for example, brands go to extreme end to appease the end user, making the customer the winner. On the other hand, hoards of both ill and well intentioned users online \"bomb\" a particular target on a daily basis.Borne out of this, is the practice of `de-biasing`, aka removing such biases from a dataset, esp while evaluating entities online that get a large number of reviewers. This project is an extension of this area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Research question(s)\n",
    "\n",
    "__Research Question 1.1: What is the impact of cognition biases amongst reviewers on the author ratings on Amazon__\n",
    "\n",
    "* __Hypothesis__: Ardent promoters and detractors for an author lead to significant inflated and deflated ratings for the author, and/or their work.\n",
    "\n",
    "__Research Question 1.2: Building upon RQ1.1, what is the scale of the distortion in user level recommendations__\n",
    "\n",
    "* __Hypothesis__: The inflated/deflated ratings for particular authors and works lead to significantly distorted recommendations at any point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data used: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial dataset is the key data source for the project. An extension of the research questions may be applied to the metadata.\n",
    "\n",
    "Note: The dataset hosted in the project will represented 5% representative sample drawn from the original dataset, after due permission from the team that worked on gathering the source dataset, i.e., Juliam McAuley et al.\n",
    "\n",
    "The data is split into two parts:\n",
    "1. __The Review Data__: The key raw fields for the reviews and ratiings\n",
    "2. __The Metadata__: Aggregated data, containing additional information about the review\n",
    "\n",
    "A third, more workable dataset called '5-core' is also part of the repository, and is more suited for the analysis:\n",
    "3. __5-core data__: These data have been reduced to extract the k-core, such that each of the remaining users and items have k reviews each.\n",
    "\n",
    "The final dataset used for evaliation for the project will only involve a subset of the data from 5-core dataset as well as the metadata dataset, with sample size < 100K reviews. This is because we further only considered books with a higher core, say with 50 reviews, done by reviewers with who’ve written 50 reviews, aka ‘50-core’. This is done on account of the heavy evaluation algorithms involved in the process and limited compute power available.\n",
    "\n",
    "Furthermore, we will only be considered with __the author, the book, the reviewer and the review score__ for the entirety of this excercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
    "    \"asin\": \"0000013714\",\n",
    "    \"reviewerName\": \"J. McDonald\",\n",
    "    \"helpful\": [2, 3],\n",
    "    \"reviewText\": \"\"\"I bought this for my husband who plays the piano. \n",
    "    He is having a wonderful time playing these old hymns.  \n",
    "    The music  is at times hard to read because we think the book was published for \n",
    "    singing from more than playing from.  Great purchase though!\"\"\",\n",
    "    \"overall\": 5.0,\n",
    "    \"summary\": \"Heavenly Highway Hymns\",\n",
    "    \"unixReviewTime\": 1252800000,\n",
    "    \"reviewTime\": \"09 13, 2009\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"asin\": \"0000031852\",\n",
    "  \"title\": \"Girls Ballet Tutu Zebra Hot Pink\",\n",
    "  \"price\": 3.17,\n",
    "  \"imUrl\": \"http://ecx.images-amazon.com/images/I/51fAmVkTbyL._SY300_.jpg\",\n",
    "  \"related\":\n",
    "  {\n",
    "    \"also_bought\": [\"B00JHONN1S\", \"B002BZX8Z6\", \"B00D2K1M3O\", \"0000031909\", \n",
    "                    \"B00613WDTQ\", \"B00D0WDS9A\", \"B00D0GCI8S\", \"0000031895\", \n",
    "                    \"B003AVKOP2\", \"B003AVEU6G\", \"B003IEDM9Q\", \"B002R0FA24\",\n",
    "                    \"B00D23MC6W\", \"B00D2K0PA0\", \"B00538F5OK\", \"B00CEV86I6\", \n",
    "                    \"B002R0FABA\", \"B00D10CLVW\", \"B003AVNY6I\", \"B002GZGI4E\", \n",
    "                    \"B001T9NUFS\", \"B002R0F7FE\", \"B00E1YRI4C\", \"B008UBQZKU\", \n",
    "                    \"B00D103F8U\", \"B007R2RM8W\"],\n",
    "    \"also_viewed\": [\"B002BZX8Z6\", \"B00JHONN1S\", \"B008F0SU0Y\", \"B00D23MC6W\", \n",
    "                    \"B00AFDOPDA\", \"B00E1YRI4C\", \"B002GZGI4E\", \"B003AVKOP2\", \n",
    "                    \"B00D9C1WBM\", \"B00CEV8366\", \"B00CEUX0D8\", \"B0079ME3KU\", \n",
    "                    \"B00CEUWY8K\", \"B004FOEEHC\", \"0000031895\", \"B00BC4GY9Y\", \n",
    "                    \"B003XRKA7A\", \"B00K18LKX2\", \"B00EM7KAG6\", \"B00AMQ17JA\", \n",
    "                    \"B00D9C32NI\", \"B002C3Y6WG\", \"B00JLL4L5Y\", \"B003AVNY6I\", \n",
    "                    \"B008UBQZKU\", \"B00D0WDS9A\", \"B00613WDTQ\", \"B00538F5OK\", \n",
    "                    \"B005C4Y4F6\", \"B004LHZ1NY\", \"B00CPHX76U\", \"B00CEUWUZC\", \n",
    "                    \"B00IJVASUE\", \"B00GOR07RE\", \"B00J2GTM0W\", \"B00JHNSNSM\", \n",
    "                    \"B003IEDM9Q\", \"B00CYBU84G\", \"B008VV8NSQ\", \"B00CYBULSO\", \n",
    "                    \"B00I2UHSZA\", \"B005F50FXC\", \"B007LCQI3S\", \"B00DP68AVW\", \n",
    "                    \"B009RXWNSI\", \"B003AVEU6G\", \"B00HSOJB9M\", \"B00EHAGZNA\", \n",
    "                    \"B0046W9T8C\", \"B00E79VW6Q\", \"B00D10CLVW\", \"B00B0AVO54\", \n",
    "                    \"B00E95LC8Q\", \"B00GOR92SO\", \"B007ZN5Y56\", \"B00AL2569W\", \n",
    "                    \"B00B608000\", \"B008F0SMUC\", \"B00BFXLZ8M\"],\n",
    "    \"bought_together\": [\"B002BZX8Z6\"]\n",
    "  },\n",
    "  \"salesRank\": {\"Toys & Games\": 211836},\n",
    "  \"brand\": \"Coxlures\",\n",
    "  \"categories\": [[\"Sports & Outdoors\", \"Other Sports\", \"Dance\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Background research and related work\n",
    "\n",
    "[De-biasing](https://en.wikipedia.org/wiki/Debiasing) is a rich research field. It is the study of techniques to detect, avoid and account for errors in judgement, intentional or unintentional. Ideally, there are three approaches to this: changing incentives, nudging, and training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ethical Considerations:\n",
    "\n",
    "Since we intend to figure out distortions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Methodology:\n",
    "\n",
    "The workload here can be shifted into \n",
    "Highlight Ardent Promoters and Detractors for each work (author-book)\n",
    "Metric: \n",
    "Reviewers that rated highly positive or very negative for more than 80% on a particular book.\n",
    "How: \n",
    "- K-Means clusters for items enough data available for 3 distinct clusters per book per author\n",
    "- Measure the changes in ratings upon removal of the highlighted book-reviewer combo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COREDATA=\"http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/Books_5.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, data_loc):\n",
    "    \"\"\" Stores and streams and\n",
    "    \"\"\"\n",
    "    \n",
    "    local_filename = data_loc+url.split('/')[-1]\n",
    "    if os.path.exists(local_filename):\n",
    "        return local_filename\n",
    "        \n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    r.close()\n",
    "    return local_filename\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        t=str(l)\n",
    "        yield json.dumps(t)\n",
    "\n",
    "def filer_man(path: str, zip_path: str, mode: str, headers: list):\n",
    "    main_dict={}\n",
    "    i=0\n",
    "    with open(path, mode) as f:\n",
    "        for line in parse(zip_path):\n",
    "            dict_line = json.loads(ast.literal_eval(eval(line)))\n",
    "            main_dict[i] = {k:v for k, v in dict_line.items() if k in headers}\n",
    "            if mode=='w':\n",
    "                f.write(line+'\\n')\n",
    "            i+=1\n",
    "    return main_dict\n",
    "\n",
    "def read_and_store(url: str, data_loc='data/', **kwargs):\n",
    "    headers = kwargs.get('headers', None)\n",
    "    \n",
    "    if not headers:\n",
    "        print(\"NEED TO SELECT HEADERS!\")\n",
    "        return None\n",
    "    \n",
    "    local_filename = download_file(url, data_loc)\n",
    "    unzip_filename = local_filename.replace('.gz', '')\n",
    "    \n",
    "    if os.path.exists(unzip_filename):\n",
    "        main_dict = filer_man(unzip_filename, local_filename, 'r', headers=[])\n",
    "        return select_df(pd.DataFrame.from_dict(main_dict, orient='index'), headers)\n",
    "    main_dict=filer_man(unzip_filename, local_filename, 'w')\n",
    "    \n",
    "    return select_df(pd.DataFrame.from_dict(main_dict, orient='index'), headers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = set(['asin', 'reviewerID', 'vote', 'overall'])\n",
    "\n",
    "file_name = \"data/Books_5.json.gz\"\n",
    "lines = (line for line in gzip.open(file_name, 'r'))\n",
    "# json_lines = (json.dumps(str(l)) for l in lines if random.random()>0.95)\n",
    "json_lines = (json.dumps(str(l)) for l in lines)\n",
    "dict_line = (json.loads(ast.literal_eval(eval(s))) for s in json_lines)\n",
    "\n",
    "final_tuple = ((item.get(k,'') for k in headers) for item in dict_line) \n",
    "f = open('data/Books_5.csv', 'a+')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(headers)\n",
    "k=0\n",
    "start_time = time.time()\n",
    "for i in final_tuple:\n",
    "    k+=1\n",
    "    if k%1e6 == 0:\n",
    "        elapsed_time = round(time.time() - start_time)\n",
    "        print(str(round(k//1e6)) + ' Mn done in ' + str(elapsed_time) + ' seconds')\n",
    "    wr.writerow(list(i))\n",
    "elapsed_time = round(time.time() - start_time)\n",
    "print('total time:' + str(elapsed_time) + ' seconds')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERREVIEWCUTOFF=7\n",
    "BOOKREVIEWCUTOFF=7\n",
    "\n",
    "countSeries = booksdf.asin.value_counts()\n",
    "booksdf['totalReviews'] = booksdf.asin.map(countSeries)\n",
    "booksdf=booksdf[booksdf.totalReviews>BOOKREVIEWCUTOFF].reset_index(drop=True)\n",
    "\n",
    "countSeries = booksdf.reviewerID.value_counts()\n",
    "booksdf['totalUserReviews'] = booksdf.reviewerID.map(countSeries)\n",
    "booksdf=booksdf[booksdf.totalUserReviews>USERREVIEWCUTOFF].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2. Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def km(X: pandas.DataFrame):\n",
    "    \"\"\"\n",
    "    K means function\n",
    "    \n",
    "    :params: X\n",
    "    \n",
    "    :returns: pandas.Series\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(X) > 3:\n",
    "        clusterer = KMeans(3)\n",
    "\n",
    "        clusterer.fit(np.array(X).reshape(-1,1))\n",
    "\n",
    "        Y=clusterer.predict(np.array(X).reshape(-1,1))\n",
    "    else:\n",
    "        return pd.Series(data=[None]*len(X),\n",
    "                         index=X)\n",
    "    \n",
    "    \n",
    "    if len(set(Y))==3:\n",
    "        return pd.Series(data=Y,\n",
    "                         index=X)\n",
    "    else:\n",
    "        return pd.Series(data=[None]*len(X),\n",
    "                         index=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerbooksdf = booksdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4773, 6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallerbooksdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0001844423</td>\n",
       "      <td>A1D2C0WDCSHUWZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0001844423</td>\n",
       "      <td>A96K1ZGW56S2I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0001844423</td>\n",
       "      <td>A1D2C0WDCSHUWZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0001048767</td>\n",
       "      <td>A1D2C0WDCSHUWZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0001048767</td>\n",
       "      <td>A2YXRT2XIJIO57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall        asin      reviewerID\n",
       "0      5.0  0001844423  A1D2C0WDCSHUWZ\n",
       "1      4.0  0001844423   A96K1ZGW56S2I\n",
       "2      5.0  0001844423  A1D2C0WDCSHUWZ\n",
       "3      5.0  0001048767  A1D2C0WDCSHUWZ\n",
       "4      5.0  0001048767  A2YXRT2XIJIO57"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallerbooksdf.drop(['vote', 'totalReviews', 'totalUserReviews'], axis=1, inplace=True)\n",
    "smallerbooksdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clusterseries = smallerbooksdf\\\n",
    "    .groupby('asin')['overall']\\\n",
    "    .apply(lambda x: km(x))\\\n",
    "    .reset_index()\\\n",
    "    .dropna(subset=['overall'])\\\n",
    "    .overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerbooksdf = smallerbooksdf.merge(clusterseries, how='left', left_index=True, right_index=True)\\\n",
    "    .rename(columns={'overall_x': 'overall', 'overall_y': 'cluster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67667, 4)"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallerbooksdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=pd.read_csv('data/meta_Books.csv', index_col='asin')\n",
    "smallerbooksdf = smallerbooksdf.merge(metadata['brand'],left_on='asin', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_check(x, check=0.8):\n",
    "    \n",
    "    cnt = len(x)\n",
    "    y = [0]*cnt\n",
    "    \n",
    "    cnt0 = len([i for i in x if i==0])\n",
    "    if cnt0 > round(check*cnt):\n",
    "        y = [1 if i==0 else i for i in y]\n",
    "    \n",
    "    cnt2 = len([i for i in x if i==2])\n",
    "    if cnt2 > round(check*cnt):\n",
    "        y = [1 if i==2 else i for i in y]\n",
    "    \n",
    "    return pd.Series(data=y,index=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkseries = smallerbooksdf.\\\n",
    "    groupby('asin')['cluster'].\\\n",
    "    apply(lambda x: user_check(x)).\\\n",
    "    reset_index().\\\n",
    "    cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerbooksdf = smallerbooksdf.merge(checkseries, left_index=True, right_index=True)\\\n",
    "    .rename(columns={'cluster_x':'cluster', 'cluster_y': 'highlight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerbooksdf.highlight = [1 if random.random() > 0.97 else 0 for i in range(smallerbooksdf.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerbooksdf.groupby(['cluster'])[['overall']].agg(['mean', 'std', 'count']).reset_index().to_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerbooksdf['asin2'] = pd.factorize(smallerbooksdf.asin)[0]+1\n",
    "smallerbooksdf['reviewerID2'] = pd.factorize(smallerbooksdf.reviewerID)[0]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. Distortion Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 0., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 3., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users=smallerbooksdf.reviewerID.unique().shape[0]\n",
    "n_items=smallerbooksdf.asin.unique().shape[0]\n",
    "\n",
    "\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in smallerbooksdf.itertuples():\n",
    "    ratings[row[8]-1, row[7]-1] = row[1]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.63%\n"
     ]
    }
   ],
   "source": [
    "sparsity = float(len(ratings.nonzero()[0]))\n",
    "sparsity /= (ratings.shape[0] * ratings.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity: {:4.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=5, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict=dict(zip(smallerbooksdf.asin2-1,\n",
    "                   smallerbooksdf.asin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T) + epsilon\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings) + epsilon\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity=fast_similarity(ratings, kind='item')\n",
    "item_similarity=fast_similarity(ratings, kind='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk(ratings, similarity, kind='user', k=40):\n",
    "    pred = np.zeros(ratings.shape)\n",
    "    if kind == 'user':\n",
    "        for i in range(ratings.shape[0]):\n",
    "            top_k_users = [np.argsort(similarity[:,i])[:-k-1:-1]]\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similarity[i, :][top_k_users].dot(ratings[:, j][top_k_users]) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[i, :][top_k_users]))\n",
    "    if kind == 'item':\n",
    "        for j in range(ratings.shape[1]):\n",
    "            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]\n",
    "            for i in range(ratings.shape[0]):\n",
    "                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items]))        \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_books(similarity, mapper, movie_idx, k=10):\n",
    "    top_books = [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]\n",
    "    top_books = set(top_books)\n",
    "    return top_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary saving file\n",
    "\n",
    "smallerbooksdf.to_csv('results/prelim_results.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerbooksdf['main_suggestions']=smallerbooksdf.asin2.\\\n",
    "    map(lambda x: top_k_books(item_similarity, mapper=movie_dict, movie_idx=x-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smallerbooksdf['second_suggestions']=smallerbooksdf.asin2.\\\n",
    "    map(lambda x: top_k_books(item_similarity2, mapper=movie_dict, movie_idx=x-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for x, y in zip(smallerbooksdf['main_suggestions'], smallerbooksdf['second_suggestions']):\n",
    "    final_results+=[1-len(x.intersection(y))/len(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.9,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.7,\n",
       " 0.7,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.19999999999999996,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.7,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.9,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.6,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.9,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.9,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.7,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 1.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 1.0,\n",
       " 0.09999999999999998,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.30000000000000004,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.6,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.30000000000000004,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.09999999999999998,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3704.,  674.,    0.,  134.,   50.,   93.,   19.,    0.,   81.,\n",
       "          20.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.hist(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11302617801047118"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15392237911087378"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Findings, implications, and limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Reflection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Justifying recommendations using distantly-labeled reviews and fined-grained aspects (Data Source)\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "_Empirical Methods in Natural Language Processing (EMNLP), 2019_, [pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
